# Databricks notebook source
# MAGIC %sql
# MAGIC --CREATE OR REPLACE TABLE dev_ml_data.complaints_articles.raw_wo_rika_limit_good AS
# MAGIC SELECT
# MAGIC   id,
# MAGIC   work_order_name,   -- Placeholder for the input column
# MAGIC   ai_query(
# MAGIC     'databricks-dbrx-instruct',
# MAGIC     CONCAT('Rewrite the following text in a more human-readable way', content)    -- Placeholder for the prompt and input
# MAGIC   ) AS content_new  -- Placeholder for the output column
# MAGIC FROM dev_ml_data.complaints_articles.c

# COMMAND ----------

wo_data = spark.sql("SELECT * FROM  `dev_ml_data`.`complaints_articles`.`raw_wo_rika_limit_good`")
display(wo_data)

# COMMAND ----------

# MAGIC %pip install --quiet mlflow==2.14.3 lxml==4.9.3 transformers==4.30.2 langchain==0.2.1 databricks-vectorsearch
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %run ../HangL/llm-rag-chatbot/_resources/00-init $reset_all_data=false

# COMMAND ----------

from langchain.text_splitter import HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter
from transformers import AutoTokenizer, OpenAIGPTTokenizer

max_chunk_size = 500

tokenizer = OpenAIGPTTokenizer.from_pretrained("openai-gpt")
text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=max_chunk_size, chunk_overlap=50)
html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=[("h2", "header2")])

# Split on H2, but merge small h2 chunks together to avoid too small. 
def split_html_on_h2(html, min_chunk_size = 20, max_chunk_size=500):
  if not html:
      return []
  h2_chunks = html_splitter.split_text(html)
  chunks = []
  previous_chunk = ""
  # Merge chunks together to add text before h2 and avoid too small docs.
  for c in h2_chunks:
    # Concat the h2 (note: we could remove the previous chunk to avoid duplicate h2)
    content = c.metadata.get('header2', "") + "\n" + c.page_content
    if len(tokenizer.encode(previous_chunk + content)) <= max_chunk_size/2:
        previous_chunk += content + "\n"
    else:
        chunks.extend(text_splitter.split_text(previous_chunk.strip()))
        previous_chunk = content + "\n"
  if previous_chunk:
      chunks.extend(text_splitter.split_text(previous_chunk.strip()))
  # Discard too small chunks
  return [c for c in chunks if len(tokenizer.encode(c)) > min_chunk_size]
 
# Let's try our chunking function
html = spark.table("dev_ml_data.complaints_articles.raw_wo_rika_limit_good").limit(1).collect()[0]['content_new']
split_html_on_h2(html)

# COMMAND ----------

VECTOR_SEARCH_ENDPOINT_NAME = 'hangle-genai'
catalog = 'dev_ml_data'
db = 'complaints_articles'

# COMMAND ----------

# MAGIC %sql
# MAGIC --Note that we need to enable Change Data Feed on the table to create the index
# MAGIC CREATE TABLE IF NOT EXISTS dev_ml_data.complaints_articles.raw_wo_rika_limit_good_chunk (
# MAGIC   id BIGINT GENERATED BY DEFAULT AS IDENTITY,
# MAGIC   work_order_name STRING,
# MAGIC   content STRING
# MAGIC ) TBLPROPERTIES (delta.enableChangeDataFeed = true); 

# COMMAND ----------

# Let's create a user-defined function (UDF) to chunk all our documents with spark
@pandas_udf("array<string>")
def parse_and_split(docs: pd.Series) -> pd.Series:
    return docs.apply(split_html_on_h2)
    
(spark.table("dev_ml_data.complaints_articles.raw_wo_rika_limit_good")
      .withColumn('content', F.explode(parse_and_split('content_new')))
      .drop("content_new")
      .write
      .option("mergeSchema", "true")
      .mode('overwrite')
      .saveAsTable('dev_ml_data.complaints_articles.raw_wo_rika_limit_good_chunk'))

# COMMAND ----------

from databricks.sdk import WorkspaceClient
import databricks.sdk.service.catalog as c
from databricks.vector_search.client import VectorSearchClient
vsc = VectorSearchClient()

# The table we'd like to index
source_table_fullname = f"{catalog}.{db}.raw_wo_rika_limit_good_chunk"
# Where we want to store our index
vs_index_fullname = f"{catalog}.{db}.raw_wo_rika_limit_good_chunk_index"

# Enable Change Data Feed on the source table
#spark.sql(f"""
#  ALTER TABLE {source_table_fullname}
#  SET TBLPROPERTIES (delta.enableChangeDataFeed = true)
#""")

if not index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname):
  print(f"Creating index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...")
  vsc.create_delta_sync_index(
    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,
    index_name=vs_index_fullname,
    source_table_name=source_table_fullname,
    pipeline_type="TRIGGERED",
    primary_key="id",
    embedding_source_column='content', # The column containing our text
    embedding_model_endpoint_name='databricks-gte-large-en' # The embedding endpoint used to create the embeddings
  )

# COMMAND ----------

  #Let's wait for the index to be ready and all our embeddings to be created and indexed
  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)
else:
  #Trigger a sync to update our vs content with the new data saved in the table
  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)
  vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).sync()

print(f"index {vs_index_fullname} on table {source_table_fullname} is ready")
